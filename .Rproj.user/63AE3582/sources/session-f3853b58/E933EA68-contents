#Regression model anaylsis part

#Task 1 - Preliminary Data Analysis Part
#This is the first part of the assignment where the dataset is loaded and the trends of the data is studied

##dataset-details
##Net hourly electrical energy (y) = x2 with other input x2: Net hourly electrical energy output (EP) in MW (dependent variable), x1: Temperature (T) – Ambient temperature (°C), x3: Ambient Pressure (AP) – Atmospheric pressure (millibar), x4: Relative Humidity (RH)– Humidity level (%), x5: Exhaust Vacuum (V) – Vacuum collected from the steam turbine (cm Hg) 
```{r}
data <- read.csv("C:/Masters Study Materials/Assignment/dataset.csv")
head(data)
```

#Create time index and select input and output columns from the data 
```{r}
t <- 1:nrow(data)
write.csv(data.frame(t = t), "t.csv", row.names = FALSE, col.names = FALSE, sep = ",")

# Extract numeric input variables and ensure they are properly typed
x <- data[, c("x1", "x3", "x4", "x5")]
x <- as.data.frame(lapply(x, as.numeric))  # Ensures all columns are numeric
write.table(x, "x.csv", row.names = FALSE, col.names = FALSE, sep = ",")

# Extract the output variable (x2) and ensure it's numeric
y <- as.numeric(data[, "x2"])
write.table(data.frame(y), "y.csv", row.names = FALSE, col.names = FALSE, sep = ",")

# Create time index and save without column name
t <- 1:nrow(data)
write.table(data.frame(t), "t.csv", row.names = FALSE, col.names = FALSE, sep = ",")

```


#Installing the required packages for the library import
```{r}
install.packages("matlib")
install.packages("rsample")
```

#importing the needed libraries
```{r}
library(matlib)
library(ggplot2)
library(rsample)
```

#importing input data from csv file
```{r}
X=as.matrix(read.csv(file="C:/Masters Study Materials/Assignment/pp_assignment/x.csv",header = F))
colnames(X)<-c("x1","x3","x4","x5")
```

#displaying imported input data
```{r}
X
```

#importing target output data from csv file
```{r}
Y=as.matrix(read.csv(file="C:/Masters Study Materials/Assignment/pp_assignment/y.csv",header = F))
colnames(Y)<-c("y")
```

#displaying targeted output data

```{r}
Y
```

#importing time series data from csv file
```{r}
time = read.csv("C:/Masters Study Materials/Assignment/pp_assignment/t.csv", header = F)
time = as.matrix(time)
```

#displaying the importe time series data
```{r}
time
```

#TASKS 

#Task 1.1 - Plotting Time Series Data

#converts x and y into time series objects for plotting
```{r}
X.ts <- ts(X, start = 1, frequency = 1)
Y.ts <- ts(Y, start = 1, frequency = 1)
```

#plotting timeseries data of input and target variable from the above time series objects
```{r}
plot(X.ts,main = "Time series plot of X Signal", xlab = "Time", ylab = "Input signal")
plot(Y.ts,main = "Time series plot of Y Signal", xlab = "Time", ylab = "Output signal")

```

# task 1.2 : Plotting distribution of each input EEG signal

#Creating a density of all input signal X
```{r}
density_of_X=density(X)
plot(density_of_X,main = "Density plot of input signal X")
```


# Creating a Histogram of X signal

```{r}
hist(X,freq = FALSE,main = "Density")
```

#combining Histogram of X signal with density plot

```{r}
hist(X,freq = FALSE,main = "Histogram and Denisty of X")
lines(density_of_X,lwd=2,col="green")
rug(jitter(X))
```

#histogram and density plot of individual input signal X and output signal y
# here we create histogram and density of each of the input signals
```{r}
#Creating a density plot of input signal X1 - Temperature(T)
density_of_X1=density(X[,"x1"])
hist(X[,"x1"],freq = FALSE,main = "Histogram and density plot of x1",xlab = "x1 Signal")
lines(density_of_X1,lwd=2,col="green")
# Add the data-points with noise in the X-axis
rug(jitter(X[,"x1"]))


#Creating a density plot of input signal X3 - Ambient Pressure(AP)
density_of_X3=density(X[,"x3"])
hist(X[,"x3"],freq = FALSE,main = "Histogram and density plot of x3",xlab = "x3 Signal")
lines(density_of_X3,lwd=2,col="green")
# Add the data-points with noise in the X-axis
rug(jitter(X[,"x3"]))


#Creating a density plot of input signal X4 - Relative Humidity(RH)
density_of_X4=density(X[,"x4"])
hist(X[,"x4"],freq = FALSE,main = "Histogram and density plot of x4",xlab = "x4 Signal")
lines(density_of_X4,lwd=2,col="green")
# Add the data-points with noise in the X-axis
rug(jitter(X[,"x4"]))

#Creating a density plot of input signal X5 - Exhaust Vacuum (V)
density_of_X5=density(X[,"x5"])
hist(X[,"x5"],freq = FALSE,main = "Histogram and density plot of x5",xlab = "x5 Signal")
lines(density_of_X5,lwd=2,col="green")
# Add the data-points with noise in the X-axis
rug(jitter(X[,"x5"]))


#Creating a density plot of output signal y - Energy Output(EP)
density_of_y=density(Y[,"y"])
hist(Y[,"y"],freq = FALSE,main = "Histogram and density plot of y",xlab = "y Signal")
lines(density_of_y,lwd=2,col="green")
# Add the data-points with noise in the X-axis
rug(jitter(Y[,"y"]))

```

#Task 1.3 Creating scatter plots to identify correlation
#arranging plot in a single screen
```{r}
par(mfrow=c(2,2)) #this sets the plotting layout to a 2x2 grid

# Plotting input signal X1 against output signal Y
plot(X[,"x1"],Y,main = "Correlation betweeen X1 and Y signal", xlab = "X1 signal", ylab = "Output signal y")

# Plotting input signal X3 against output signal Y
plot(X[,"x3"],Y,main = "Correlation betweeen X3 and Y signal", xlab = "X3 signal", ylab = "Output signal y")

# Plotting input signal X4 against output signal Y
plot(X[,"x4"],Y,main = "Correlation betweeen X4 and Y signal", xlab = "X4 signal", ylab = "Output signal y")

# Plotting input signal X5 against output signal Y
plot(X[,"x5"],Y,main = "Correlation betweeen X5 and Y signal", xlab = "X5 signal", ylab = "Output signal y")

```

# Task 2: Regression – modelling the relationship between gene expression  
# Calculating ones for binding the data
```{r}
ones = matrix(1 , length(X)/4,1)
ones
```

# Task 2.1
# Calculating thetahat of all the given models
```{r}
# Set Model 1 predictors
X_model1_raw <- cbind(X[,"x4"], X[,"x3"]^2)

# Scale predictors (not the intercept) 
X_model1_scaled <- scale(X_model1_raw)

# Add intercept back
X_model1 <- cbind(ones, X_model1_scaled)

head(X_model1)

#Calculating thetahat of Model 1
Model1_thetahat=solve(t(X_model1) %*% X_model1) %*% t(X_model1) %*% Y
Model1_thetahat


#For model 2
#Binding data from equation of model 2.
# Model 2 predictors
X_model2_raw <- cbind(X[,"x4"],X[,"x3"]^2,X[,"x5"])

# Scale predictors (not the intercept)
X_model2_scaled <- scale(X_model2_raw)

# Add intercept back
X_model2 <- cbind(ones, X_model2_scaled)

head(X_model2)

#Calculating thetahat of Model 2
Model2_thetahat=solve(t(X_model2) %*% X_model2) %*% t(X_model2) %*% Y
Model2_thetahat

#Model 3

#Binding data from equation of model 3.

# Model 3 predictors
X_model3_raw <- cbind(X[,"x3"],X[,"x4"],X[,"x5"]^3)

# Scale predictors (not the intercept)
X_model3_scaled <- scale(X_model3_raw)

# Add intercept back
X_model3 <- cbind(ones, X_model3_scaled)

head(X_model3)

#Calculating thetahat of Model 3
Model3_thetahat=solve(t(X_model3) %*% X_model3) %*% t(X_model3) %*% Y
Model3_thetahat



#For model 4
#Binding data from equation of model 4.

# Model 4 predictors
X_model4_raw <- cbind(X[,"x4"],(X[,"x3"])^2,(X[,"x5"])^3)

# Scale predictors (not the intercept)
X_model4_scaled <- scale(X_model4_raw)

# Add intercept back
X_model4 <- cbind(ones, X_model4_scaled)

head(X_model4)

#Calculating thetahat of Model 4
Model4_thetahat=solve(t(X_model4) %*% X_model4) %*% t(X_model4) %*% Y
Model4_thetahat


# for Model 5
#Binding data from equation of model 5.

# Model 5 predictors
X_model5_raw <- cbind((X[,"x4"]),(X[,"x1"])^2,(X[,"x3"])^2)

# Scale predictors (not the intercept)
X_model5_scaled <- scale(X_model5_raw)

# Add intercept back
X_model5 <- cbind(ones, X_model5_scaled)

head(X_model5)

#Calculating thetahat of Model 5
Model5_thetahat=solve(t(X_model5) %*% X_model5) %*% t(X_model5) %*% Y
Model5_thetahat

```


#displaying thetahat values of each model
```{r}
#model1
Model1_thetahat
t(Model1_thetahat)
#model 2
Model2_thetahat
t(Model2_thetahat)
#model 3
Model3_thetahat
t(Model3_thetahat)
#model 4
Model4_thetahat
t(Model4_thetahat)
#model 5
Model5_thetahat
t(Model5_thetahat)
```
```{r}
# Models learnings from the thetahat -

#Model 1: Utilizes pressure squared and humidity → Pressure squared decreases energy output, while humidity enhances it.

#Model 2: Vacuum is added to Model 1 → The same as before, but there is also a slight benefit to vacuum.

#Model 3: Utilizes vacuum cubed, pressure, and humidity While pressure still reduces output, vacuum cubed helps increase it.

#Model 4: Utilizes vacuum cubed, pressure squared, and humidity → Whereas pressure squared reduces production, humidity and vacuum squared aid.

#Model 5: Makes use of temperature squared, pressure squared, and humidity. humidity helps
```


#Task 2.2 - modal residual sum of squared errors (RSS Calculation)
#Calculating Y-hat and RSS for each model
```{r}
#Calculating Y-hat and RSS of Model 1
Y_hat_model1 = X_model1 %*% Model1_thetahat
head(Y_hat_model1)
#Calculating RSS
RSS_Model_1=sum((Y-Y_hat_model1)^2)
head(RSS_Model_1)

# Calculating Y-hat and RSS of Model 2
Y_hat_model2 = X_model2 %*% Model2_thetahat
head(Y_hat_model2)
#Calculating RSS
RSS_Model_2=sum((Y-Y_hat_model2)^2)
head(RSS_Model_2)

# Calculating Y-hat and RSS of Model 3
Y_hat_model3 = X_model3 %*% Model3_thetahat
head(Y_hat_model3)
#Calculating RSS
RSS_Model_3=sum((Y-Y_hat_model3)^2)
head(RSS_Model_3)
 
# Calculating Y-hat and RSS of Model 4
Y_hat_model4 = X_model4 %*% Model4_thetahat
head(Y_hat_model4)
#Calculating RSS
RSS_Model_4=sum((Y-Y_hat_model4)^2)
head(RSS_Model_4)

# Calculating Y-hat and RSS of Model 5
Y_hat_model5 = X_model5 %*% Model5_thetahat
head(Y_hat_model5)
#Calculating RSS
RSS_Model_5=sum((Y-Y_hat_model5)^2)
head(RSS_Model_5)
```

#printing RSS value

```{r}
model1 <- c(RSS_Model_1)
model2 <- c(RSS_Model_2)
model3 <- c(RSS_Model_3)
model4 <- c(RSS_Model_4)
model5 <- c(RSS_Model_5) #model 5 has the lowest RSS of all the models shown and is most likely the best fix

dfRSS <- data.frame(model1, model2,model3,model4,model5)
dfRSS

```


#Task 2.3 Calculating the log likelihood and Variance of each model
#calculating the variance to see how spread out our model prediction are and log likelihood is the measure of how likely our model would produce the data we observed
```{r}
#Getting the number of observations for variance model
N=length(Y)

#Calculating Model 1 Variance and log-likelihood
Variance_model1=RSS_Model_1/(N-1)
Variance_model1
likehood_Model_1=
  -(N/2)*(log(2*pi))-(N/2)*(log(Variance_model1))-(1/(2*Variance_model1))*RSS_Model_1
likehood_Model_1

#Calculating Model 2 Variance and log-likelihood
Variance_model2=RSS_Model_2/(N-1)
Variance_model2
likehood_Model_2=
  -(N/2)*(log(2*pi))-(N/2)*(log(Variance_model2))-(1/(2*Variance_model2))*RSS_Model_2
likehood_Model_2

#Calculating Model 3 Variance and log-likelihood
Variance_model3=RSS_Model_3/(N-1)
Variance_model3
likehood_Model_3=
  -(N/2)*(log(2*pi))-(N/2)*(log(Variance_model3))-(1/(2*Variance_model3))*RSS_Model_3
likehood_Model_3

#Calculating Model 4 Variance and log-likelihood
Variance_model4=RSS_Model_4/(N-1)
Variance_model4
likehood_Model_4=
  -(N/2)*(log(2*pi))-(N/2)*(log(Variance_model4))-(1/(2*Variance_model4))*RSS_Model_4
likehood_Model_4

#Calculating Model 5 Variance and log-likelihood
Variance_model5=RSS_Model_5/(N-1)
Variance_model5
likehood_Model_5=
  -(N/2)*(log(2*pi))-(N/2)*(log(Variance_model5))-(1/(2*Variance_model5))*RSS_Model_5
likehood_Model_5
```

#displaying variance values
```{r}
model1 <- c(Variance_model1)
model2 <- c(Variance_model2)
model3 <- c(Variance_model3)
model4 <- c(Variance_model4)
model5 <- c(Variance_model5)

dfVariance <- data.frame(model1, model2,model3,model4,model5)
dfVariance 

# model 5 has the lowest variance observed
```

#displaying likelihood values of all the models
```{r}
model1 <- c(likehood_Model_1)
model2 <- c(likehood_Model_2)
model3 <- c(likehood_Model_3)
model4 <- c(likehood_Model_4)
model5 <- c(likehood_Model_5)

dfLikelihood <- data.frame(model1, model2,model3,model4,model5)
dfLikelihood
#Model 5 is the closes to 0 hence making it the best model up to now
```

# Task 2.4 - Compute Akaike information criterion (AIC) and Baysesian information criterion(BIC) of all the models

#Calculating AIC And BIC of each model
#AIC and BIC are both model selection criteria. These help us to figure out which model is suited better among all the models
```{r}
# Calculating AIC and BIC of model 1

#thetahat of the models contain the estimated regression coefficients for Models — including:
#1 intercept (added by ones)
#And the coefficients for the predictors
#So, length(ModelX_thetahat) gives K, the total number of parameters


K_model1<-length(Model1_thetahat)
K_model1
AIC_model1=2*K_model1-2*likehood_Model_1
AIC_model1
BIC_model1=K_model1*log(N)-2*likehood_Model_1
BIC_model1

## thetahat of model 2
K_model2<-length(Model2_thetahat)
K_model2
##Calculating AIC and BIC of model 2
AIC_model2=2*K_model2-2*likehood_Model_2
AIC_model2
BIC_model2=K_model2*log(N)-2*likehood_Model_2
BIC_model2

## thetahat of model 3
K_model3<-length(Model3_thetahat)
K_model3
##Calculating AIC and BIC of model 3
AIC_model3=2*K_model3-2*likehood_Model_3
AIC_model3
BIC_model3=K_model3*log(N)-2*likehood_Model_3
BIC_model3

## thetahat of model 4
K_model4<-length(Model4_thetahat)
K_model4
##Calculating AIC and BIC of model 4
AIC_model4=2*K_model4-2*likehood_Model_4
AIC_model4
BIC_model4=K_model4*log(N)-2*likehood_Model_4
BIC_model4

## thetahat of model 5
K_model5<-length(Model5_thetahat)
K_model5
##Calculating AIC and BIC of model 5
AIC_model5=2*K_model5-2*likehood_Model_5
AIC_model5
BIC_model5=K_model5*log(N)-2*likehood_Model_5
BIC_model5
```

#Displaying K values from above
```{r}
model1 <- c(K_model1)
model2 <- c(K_model2)
model3 <- c(K_model3)
model4 <- c(K_model4)
model5 <- c(K_model5)

dfK <- data.frame(model1, model2,model3,model4,model5)
dfK
```

#Dispalying the AIC values
```{r}
model1 <- c(AIC_model1)
model2 <- c(AIC_model2)
model3 <- c(AIC_model3)
model4 <- c(AIC_model4)
model5 <- c(AIC_model5)

dfAIC <- data.frame(model1, model2,model3,model4,model5)
dfAIC

#Model 5 is the best fit according to the AIC as it has the lowest value(62108.79)
```

#Displaying the BIC values
```{r}
model1 <- c(BIC_model1)
model2 <- c(BIC_model2)
model3 <- c(BIC_model3)
model4 <- c(BIC_model4)
model5 <- c(BIC_model5)

dfBIC <- data.frame(model1, model2,model3,model4,model5)
dfBIC
#model 5 is the best fit according to BIC as it has the lowest value
```

## Task 2.5 Check error plotting normal/gaussian distibution of each plot
```{r}
par(mfrow=c(1,1)) #sets grid of 1x1

## Error of model1
model1_error <- Y-Y_hat_model1
model1_error

## Plotting the graph QQplot and QQ line of model 1
qqnorm(model1_error, col = "darkgreen",main = "QQ plot of model 1")
qqline(model1_error, col = "orange",lwd=1)


## Error of model2
model2_error <- Y-Y_hat_model2 # error of model 2
## Plotting QQplot and QQ line of model 2
qqnorm(model2_error, col = "darkgreen",main = "QQ plot of model 2")
qqline(model2_error, col = "orange")


## Error of model3
model3_error <- Y- Y_hat_model3
## Plotting QQplot and QQ line of model 3
qqnorm(model3_error, col = "darkgreen",main = "QQ plot of model 3")
qqline(model3_error, col = "orange")

## Error of model4
model4_error <- Y-Y_hat_model4
## Plotting QQplot and QQ line of model 4
qqnorm(model4_error, col = "darkgreen",main = "QQ plot of model 4")
qqline(model4_error, col = "orange")

## Error of model5
model5_error <- Y- Y_hat_model5
## Plotting QQplot and QQ line of model 5
qqnorm(model5_error, col = "darkgreen",main = "QQ plot of model 5")
qqline(model5_error, col = "orange")


## Quantile Quantile plots tells that the model 5 is the best as the residuals clearly follow the line with only slight deviations at the endl
```

#Task 2.6 Select ‘best’ regression model according to the AIC, BIC and distribution of model residuals 
```{r}
##The best model up to now is model 5 as according to the AIC it had the lowest value ,
#according to BIC it is model 5 and according to distribution of model residuals (QQ Plot) it is still model 5.
```


#Task 2.7 Splitting the input and output dataset (X and y) into two parts: one part used to train the model, the other used for testing (e.g. 70% for training, 30% for testing)

#also plotting normal distribution graph of training data
```{r}
## Splitting the data set y into Training and testing data set.
split_Y<-initial_split(data = as.data.frame(Y),prop=.7)
## Training split Y data set 
Y_training_set<-training(split_Y)
Y_testing_set<-as.matrix(testing(split_Y))

Y_training_data<-as.matrix(Y_training_set)

## Splitting the data set of X into Training and testing data set.
split_X<-initial_split(data = as.data.frame(X),prop=.7)
## Training split X data set
X_training_set<-training(split_X)
## Testing split Y data set 
X_testing_set<-as.matrix(testing(split_X))
X_testing_data<-as.matrix(X_testing_set)
X_training_data<-as.matrix(X_training_set)

# Create raw training predictors for model 5
X_train_model_raw <- cbind(
  x4 = X_training_set[,"x4"],
  x1_sq = X_training_set[,"x1"]^2,
  x3_sq = X_training_set[,"x3"]^2
)

# Apply scaling
X_train_scaled <- scale(X_train_model_raw)

# Save scaling parameters
center_vals <- attr(X_train_scaled, "scaled:center")
scale_vals  <- attr(X_train_scaled, "scaled:scale")

# Create final training matrix with intercept
training_ones <- matrix(1, nrow(X_train_scaled), 1)
X_traning_model <- cbind(training_ones, X_train_scaled)

# Estimate thetahat
traning_thetahat <- solve(t(X_traning_model) %*% X_traning_model) %*% 
                    t(X_traning_model) %*% Y_training_data

# === Apply same scaling to test set === #
X_test_model_raw <- cbind(
  x4 = X_testing_set[,"x4"],
  x1_sq = X_testing_set[,"x1"]^2,
  x3_sq = X_testing_set[,"x3"]^2
)

# Apply the same centering and scaling from training set
X_test_scaled <- scale(X_test_model_raw, center = center_vals, scale = scale_vals)

# Final test matrix
test_ones <- matrix(1, nrow(X_test_scaled), 1)
X_test_model <- cbind(test_ones, X_test_scaled)

### Estimating model parameters using Training set
#training_ones=matrix(1 , length(X_training_set$x1),1)
# selected model 2 and using equation of model 5
#X_training_model<-cbind(training_ones,X_training_set[,"x4"],(X_training_set[,"x1"])^2,(X_training_set[,"x3"])^2)

traning_thetahat=solve(t(X_traning_model) %*% X_traning_model) %*% t(X_traning_model) %*%  Y_training_data
  
### Model out/Prediction
Y_testing_hat = X_test_model  %*% traning_thetahat
head(Y_testing_hat)
RSS_testing <- sum((Y_testing_set-Y_testing_hat)^2)
head(RSS_testing)
t.test(X_traning_model, mu=500, alternative="two.sided", conf.level=0.95)
C_I1=-0.2783950
C_I2=0.2762101
p2 <- plot(density(X_traning_model), col="green", lwd=2,
           main="Distribution of Traning Data")
abline(v=C_I1,col="orange", lty=2)
abline(v=C_I2,col="orange", lty=2)

thetaHat_training =solve(t(X_training_data) %*% X_training_data) %*% t(X_training_data) %*%Y_training_data
thetaHat_training
length(thetaHat_training)
dis_test=density(Y_training_data)
plot((dis_test))
plot(dis_test,main = "Density plot of Y Signal")

### Calculating Confidential interval
z=1.96 ##(95%) Confidential interval
error=((Y_testing_set-Y_testing_hat))
n_len=length(Y_testing_hat)
C_I_1= z * sqrt( (error * (1-error) ) / n_len)
C_I_1
error
C_I_2= z * sqrt( (error * (1+error)) / n_len)
C_I_2            

```

#Task 3: Approximate Bayesian Computation (ABC)  
```{r}
## Model 5 will be used, parameter are selected and kept constant.
arr_1=0
arr_2=0
f_value=0
s_value=0
Model5_thetahat
#values from thetahat
thetebias <- 454.365009 #selected parameter
thetaone <- 1.349107 # selected parameter
thetatwo <- -10.605331 # constant value
thetathree <- -5.173124 # constant value


Epison <- RSS_Model_5 * 2 ## fixing value of eplision
num <- 100 #number of iteration
##Calculating Y-hat for performing rejection ABC
counter <- 0
for (i in 1:num) {
  range1 <- runif(1, -454.365009, 454.365009) # calculating the range
  range1
  range2 <- runif(1, -1.349107, 1.349107)
  New_thetahat <- matrix(c(range1,range2,thetatwo,thetathree))
  New_Y_Hat <- X_model5 %*% New_thetahat ## calculating new Y-hat
  new_RSS <- sum((Y-New_Y_Hat)^2)
  new_RSS
  if (new_RSS > Epison){
    arr_1[i] <- range1
    arr_2[i] <- range2
    counter = counter+1
    f_value <- matrix(arr_1)
    s_value <- matrix(arr_2)
  }
}
hist(f_value)
hist(s_value)

###ploting Joint and Marginal Posterior Distribution of the graph
plot(f_value,s_value, col = c("orange", "green"), main = "Joint and Marginal Posterior Distribution")
par(mfrow=c(1,1))
```

